# -*- coding: utf-8 -*-
"""SAECASM _DETECTION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EmqW1UJ_iI7UaVv4uLWsVSdV2nv4ykuO
"""

import json
import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from nltk.stem.wordnet import WordNetLemmatizer
import re, string
from nltk.corpus import stopwords

from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ReduceLROnPlateau

import warnings
warnings.filterwarnings("ignore")

df_News = pd.read_json('/content/Sarcasm_Headlines_Dataset.json', lines=True)

df_News.head()

df_News = df_News.drop(columns=['article_link'])

df_News.head()

lem = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))
punctuations = string.punctuation

def clean_text(news):
    """
    This function receives headlines sentence and returns clean sentence
    """
    news = news.lower()
    words = list(news.split())

    words = [lem.lemmatize(word, "v") for word in words]
    words = [w for w in words if w not in punctuations]
    return " ".join(words)

df_News['news_headline'] = df_News.headline.apply(lambda news: clean_text(news))

df_News.head()

df_News['headline_len'] = df_News['news_headline'].apply(lambda headline: len(headline.split()))

df_News.groupby(['is_sarcastic'])['headline_len'].mean()

df_News.groupby(['is_sarcastic']).headline_len.max()