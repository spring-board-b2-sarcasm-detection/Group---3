{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjyQf7xUkze4"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87DeMzp-k3L5"
   },
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYNAaUrYg8mQ"
   },
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_length = max([len(x) for x in train_sequences])  # Find the maximum sequence length\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Pxl3EGvhA49"
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [sentence.split() for sentence in train_data]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2Lb4mU0hC6A"
   },
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2UKcxgdhfTO"
   },
   "outputs": [],
   "source": [
    "# Convert sequences to Word2Vec embeddings\n",
    "def convert_to_word2vec(sequences, embedding_matrix, max_length):\n",
    "    word2vec_sequences = np.zeros((len(sequences), max_length, embedding_dim))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j, word_index in enumerate(sequence):\n",
    "            if word_index != 0:  # Skip padding\n",
    "                word_vec = embedding_matrix[word_index]\n",
    "                word2vec_sequences[i, j] = word_vec\n",
    "    return word2vec_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYsz4zwEhiQn"
   },
   "outputs": [],
   "source": [
    "train_word2vec = convert_to_word2vec(train_padded, embedding_matrix, max_length)\n",
    "test_word2vec = convert_to_word2vec(test_padded, embedding_matrix, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2nGLyYChjM3",
    "outputId": "9a4e602a-0bf1-4454-d519-6d95d95a7b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_word2vec: (22802, 106, 100)\n",
      "Shape of test_word2vec: (5701, 106, 100)\n",
      "Sample Word2Vec encoded train sequence:\n",
      " [[-0.72729731  1.41205347  0.6908955  ... -1.16739058  0.14696582\n",
      "   0.66753608]\n",
      " [-0.19192421  0.36437336  0.18654716 ... -0.30756003  0.04675481\n",
      "   0.16850248]\n",
      " [-0.02013207  0.05818575  0.03467382 ... -0.04615525 -0.00236784\n",
      "   0.02701318]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Sample Word2Vec encoded test sequence:\n",
      " [[-0.04658628  0.0851171   0.04052103 ... -0.06102676  0.01058459\n",
      "   0.02753298]\n",
      " [-0.06167693  0.11658745  0.04834437 ... -0.09984828  0.02152216\n",
      "   0.04867287]\n",
      " [-0.00674185 -0.00117609  0.00085781 ... -0.00459011  0.00280366\n",
      "   0.00420339]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect Word2Vec encoded sequences\n",
    "print(\"Shape of train_word2vec:\", train_word2vec.shape)\n",
    "print(\"Shape of test_word2vec:\", test_word2vec.shape)\n",
    "print(\"Sample Word2Vec encoded train sequence:\\n\", train_word2vec[0])\n",
    "print(\"Sample Word2Vec encoded test sequence:\\n\", test_word2vec[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
