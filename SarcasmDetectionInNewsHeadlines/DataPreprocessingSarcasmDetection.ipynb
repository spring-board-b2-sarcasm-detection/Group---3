{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5yRtrxwBUhB",
        "outputId": "5f8cda2b-4a0f-4ec0-8c77-84cff8b9855c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5BWilwC1dx",
        "outputId": "9aa155c0-27c9-4f7b-f011-a901947cf56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stop_words(words):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return filtered_words"
      ],
      "metadata": {
        "id": "HKEKte-RSMom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "tF3OdWwUfUgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/gdrive/MyDrive/Sarcasm_Headlines_Dataset.json'\n",
        "raw_data_json = pd.read_json(path, lines=True)\n",
        "\n",
        "print(\"Raw data is :\\n\\n\", raw_data_json.loc[:20,['is_sarcastic','headline']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8gT8XFEDKZo",
        "outputId": "965a0a73-9b06-494e-c741-4738e467d9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data is :\n",
            "\n",
            "     is_sarcastic                                           headline\n",
            "0              1  thirtysomething scientists unveil doomsday clo...\n",
            "1              0  dem rep. totally nails why congress is falling...\n",
            "2              0  eat your veggies: 9 deliciously different recipes\n",
            "3              1  inclement weather prevents liar from getting t...\n",
            "4              1  mother comes pretty close to using word 'strea...\n",
            "5              0                               my white inheritance\n",
            "6              0         5 ways to file your taxes with less stress\n",
            "7              1  richard branson's global-warming donation near...\n",
            "8              1  shadow government getting too large to meet in...\n",
            "9              0                 lots of parents know this scenario\n",
            "10             0  this lesbian is considered a father in indiana...\n",
            "11             0  amanda peet told her daughter sex is 'a specia...\n",
            "12             0  what to know regarding current treatments for ...\n",
            "13             0  chris christie suggests hillary clinton was to...\n",
            "14             1  ford develops new suv that runs purely on gaso...\n",
            "15             0  uber ceo travis kalanick stepping down from tr...\n",
            "16             1  area boy enters jumping-and-touching-tops-of-d...\n",
            "17             1      area man does most of his traveling by gurney\n",
            "18             0           leave no person with disabilities behind\n",
            "19             0  lin-manuel miranda would like to remind you to...\n",
            "20             0  60 journalists killed in 2014 as targeting of ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "special_characters = \"!@#$%^&*()-_+=[]{}|:;\\\"'<>,.?/\"\n",
        "\n",
        "for index, elem in raw_data_json.iterrows():\n",
        "  text = elem['headline']\n",
        "  outcome = elem['is_sarcastic']\n",
        "\n",
        "  x=[]\n",
        "  str1 = \"\"\n",
        "  for char in (text + ' '):\n",
        "    if char == ' ' or char == '-' or char == '_':\n",
        "      x.append(str1)\n",
        "      str1 = \"\"\n",
        "    elif char not in special_characters:\n",
        "      str1 = str1 + char\n",
        "\n",
        "  if len(x) > 0:\n",
        "    filtered_x = filter_stop_words(x)\n",
        "  else:\n",
        "    continue\n",
        "  if len(filtered_x) > 0:\n",
        "    lemmatized_x = lemmatize_words(filtered_x)\n",
        "    X.append(lemmatized_x)\n",
        "    Y.append(outcome)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L_baj0fEIwTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert (len(X) == len(Y))\n",
        "\n",
        "sarcastic_data = []\n",
        "non_sarcastic_data = []\n",
        "\n",
        "n_X = len(X)\n",
        "\n",
        "for i in range(n_X):\n",
        "  if Y[i] == 1:\n",
        "    sarcastic_data.append(X[i])\n",
        "  else:\n",
        "    non_sarcastic_data.append(X[i])\n",
        "\n",
        "print(\"Number of sarcastic samples : \" + str(len(sarcastic_data)) + \"\\nNumber of non sarcastic samples : \" + str(len(non_sarcastic_data)))\n",
        "\n",
        "\n",
        "seed_value = 42\n",
        "split_at = 9544 # decides number of sarcastic and non-sarcastic data in train set\n",
        "n_sarcastic = len(sarcastic_data)\n",
        "n_non_sarcastic = len(non_sarcastic_data)\n",
        "random.seed(seed_value)\n",
        "\n",
        "\n",
        "\n",
        "train_data = sarcastic_data[:split_at] + non_sarcastic_data[:split_at]\n",
        "train_labels = ([1] * split_at) + ([0] * split_at)\n",
        "test_data = (sarcastic_data[split_at:] + non_sarcastic_data[split_at:])\n",
        "test_labels = ([1] * (n_sarcastic - split_at)) + ([0] * (n_non_sarcastic -split_at))\n",
        "\n",
        "n_train = len(train_data)\n",
        "n_test = len(test_data)\n",
        "\n",
        "assert (len(train_data) == len(train_labels))\n",
        "assert (len(test_data) == len(test_labels))\n",
        "\n",
        "combined_train_data = list(zip(train_data, train_labels))\n",
        "combined_test_data = list(zip(test_data, test_labels))\n",
        "random.shuffle(combined_train_data)\n",
        "random.shuffle(combined_test_data)\n",
        "\n",
        "X_train, Y_train = zip(*combined_train_data) # Training set\n",
        "X_test, Y_test = zip(*combined_test_data) # Test set\n",
        "\n",
        "print(\"Length of training set is : \", n_train)\n",
        "print(\"Length of test set is : \", n_test)\n",
        "\n",
        "for i in range(100):\n",
        "  print(X_train[i], Y_train[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_y3AUIoX1gA",
        "outputId": "a1a6ea2d-e304-454a-d470-1a348007e807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sarcastic samples : 13634\n",
            "Number of non sarcastic samples : 14984\n",
            "Length of training set is :  19088\n",
            "Length of test set is :  9530\n",
            "['christmas', 'pageant', 'enters', 'pre', 'production'] 1\n",
            "['unidentified', 'yowling', 'animal', 'carrier', 'apparently', 'named', 'kiwi'] 1\n",
            "['study', 'every', '10', 'second', 'skyscraper', 'window', 'washer', 'fall', 'death'] 1\n",
            "['paranoid', 'oscar', 'pistorius', 'still', 'think', 'burglar'] 1\n",
            "['new', 'war', 'enables', 'mankind', 'resolve', 'disagreement'] 1\n",
            "['strange', 'looking', 'pup', 'find', 'perfect', 'family'] 0\n",
            "['mlb', 'player', 'yordano', 'ventura', 'andy', 'marte', 'die', 'separate', 'car', 'crash'] 0\n",
            "['science', 'behind', 'celebrity', 'like', 'ryan', 'lochte', 'tell', 'fib'] 0\n",
            "['voice', 'recognition', 'software', 'yelled'] 1\n",
            "['flooding', 'texas', 'lead', 'mosquito', 'borne', 'illness'] 0\n",
            "['ebola', 'sierra', 'leone', 'reminded', 'conflict', 'zone'] 0\n",
            "['podiatrist', 'recommend', 'getting', 'foot', 'rotated', 'every', '6', 'month'] 1\n",
            "['new', 'gun', 'law', 'would', 'require', 'james', 'holmes', 'undergo', 'strict', 'background', 'check', 'purchasing', 'firearm'] 1\n",
            "['trump', 'asks', 'entire', 'senate', 'clear', 'chamber', 'speak', 'comey', 'alone'] 1\n",
            "['first', 'baby', '2010', 'finally', 'born'] 1\n",
            "['activist', 'rally', 'domestic', 'violence', 'survivor', 'found', 'guilty', 'child', 'abduction'] 0\n",
            "['ira', 'hamas', 'sweep', '1990', 'bombie', 'award'] 1\n",
            "['hillary', 'clinton', 'making', 'big', 'promise', 'ufo', 'believer'] 0\n",
            "['disaster', 'strike', 'mother', 'newborn', 'vulnerable'] 0\n",
            "['new', 'wondersplint', 'make', 'fracture', 'appear', 'larger', 'fuller'] 1\n",
            "['grieving', 'couple', 'find', 'different', 'way', 'use', 'stroller'] 1\n",
            "['man', 'unable', 'explain', 'contempt', 'feel', 'group', 'people', 'enjoying', 'one', 'anothers', 'company'] 1\n",
            "['gun', 'ultimate', 'public', 'health', 'crisis', 'howard', 'dean', 'tell', 'democratic', 'convention'] 0\n",
            "['project', '24', 'portrait', 'millennial', 'artist', 'andrew', 'kaminski'] 0\n",
            "['zales', 'introduces', 'new', 'line', 'casual', 'dating', 'diamond', 'ring'] 1\n",
            "['trump', 'new', 'medicaid', 'rule', 'arent', 'empowering', 'people', 'theyre', 'punishing', 'poor'] 0\n",
            "['8', 'reason', 'woman', 'midlife', 'need', 'besties'] 0\n",
            "['pro', 'israel', 'mean'] 0\n",
            "['largest', 'demographic', 'binge', 'drinker', 'might', 'surprise'] 0\n",
            "['dole', 'make', 'pretend', 'white', 'house', 'card', 'table', 'sheet'] 1\n",
            "['colorful', 'multicultural', 'mural', 'celebrates', 'diverse', 'lack', 'talent'] 1\n",
            "['historian', 'still', 'unable', 'determine', 'american', 'able', 'build', 'hoover', 'dam'] 1\n",
            "['newspaper', 'formally', 'apologizes', 'wookiees', '40', 'year', 'old', 'star', 'war', 'mistake'] 0\n",
            "['2', 'year', 'old', 'lifetime', 'worth', 'perfect', 'halloween', 'costume'] 0\n",
            "['backstreet', 'boy', 'become', 'backstreet', 'men', 'backstreet', 'ritual'] 1\n",
            "['unexpected', 'heirloom'] 0\n",
            "['anonymous', 'source', 'informs', 'bob', 'woodward', 'hasnt', 'relevant', '40', 'year'] 1\n",
            "['strangely', 'compelling', 'shybot', 'roams', 'california', 'desert', 'avoiding', 'human'] 0\n",
            "['black', 'ribbon', 'balsam'] 0\n",
            "['oyster', 'discernible', 'effect', 'date'] 1\n",
            "['applebees', 'offer', 'divorced', 'father', 'child', 'special', 'every', 'weekend'] 1\n",
            "['egypt', 'set', 'date', 'parliamentary', 'election'] 0\n",
            "['confusing', 'roadside', 'memorial', 'feature', 'bicycle', 'rotary', 'telephone', 'jug', 'kind'] 1\n",
            "['new', 'dating', 'website', 'help', 'plus', 'size', 'jewish', 'plane', 'crash', 'survivor', 'find', 'love'] 1\n",
            "['private', 'prison', 'problem'] 0\n",
            "['hawaii', 'millennials', 'day'] 0\n",
            "['new', 'chapter', 'u', 'cuba', 'relation'] 0\n",
            "['house', 'bipartisanship', 'throw', 'pitifully', 'weak', 'toxic', 'chemical', 'control', 'bill'] 0\n",
            "['playground', 'treated', 'hot', 'pug', 'pug', 'action'] 1\n",
            "['lazy', 'poor', 'person', 'never', 'earned', 'passive', 'income', 'stock', 'dividend', 'day', 'life'] 1\n",
            "['mattis', 'tillerson', 'want', 'blank', 'check', 'wage', 'illegal', 'war'] 0\n",
            "['processing', 'fact', 'fergusons', 'legacy'] 0\n",
            "['right', 'live', 'life', 'complete', 'stunned', 'horror', 'added', 'constitution'] 1\n",
            "['obamas', 'declaration', 'swine', 'flu', 'emergency', 'prompt', 'pro', 'swine', 'flu', 'republican', 'response'] 1\n",
            "['glenn', 'close', 'angry', 'darkly', 'sad', 'harvey', 'weinstein', 'allegation'] 0\n",
            "['meet', 'eye', 'tony', 'award', 'frontrunner', 'christopher', 'jackson', 'hamilton'] 0\n",
            "['eye', 'removed', 'violent', 'yearbook', 'attack'] 1\n",
            "['kfc', 'release', 'new', 'family', 'size', 'nugget'] 1\n",
            "['democratic', 'congressman', 'protest', 'trump', 'environmental', 'policy', 'bringing', 'endangered', 'red', 'wolf', 'state', 'union', 'guest'] 1\n",
            "['25', 'year', 'old', 'man', 'longer', 'impressed', 'mewtwo'] 1\n",
            "['study', 'beginning', 'email', 'short', 'disingenuous', 'inquiry', 'personal', 'life', 'best', 'way', 'network'] 1\n",
            "['dr', 'oz', 'explains', 'men', 'rarely', 'address', 'mental', 'health', 'issue'] 0\n",
            "['north', 'korea', 'claim', 'new', 'long', 'range', 'missile', 'ability', 'fly', 'right', 'air', 'unlike', 'bird', 'fly'] 1\n",
            "['new', 'mit', 'study', 'suggests', 'sonic', 'hedgehog', 'might', 'living', 'computer', 'simulation'] 1\n",
            "['supreme', 'court', 'understudy', 'fill', 'scalia'] 1\n",
            "['flash', 'animated', 'osama', 'bin', 'laden', 'captured'] 1\n",
            "['biden', 'request', 'named', 'special', 'envoy', 'reno'] 1\n",
            "['maya', 'angelou', 'thought', 'shed', 'invited', 'white', 'house', 'stuff'] 1\n",
            "['anti', 'homosexuality', 'sermon', 'suspiciously', 'well', 'informed'] 1\n",
            "['state', 'help', '5', 'million', 'kid', 'parent', 'behind', 'bar'] 0\n",
            "['shooting', 'non', 'target'] 0\n",
            "['le', 'mis√©rables', 'take', 'home', 'oscar', 'sound'] 1\n",
            "['executive', 'legislative', 'judicial', 'branch', 'merge'] 1\n",
            "['plea', 'free', 'archbishop', 'mar', 'gregorios', 'yohanna', 'ibrahim', 'archbishop', 'boulos', 'yazigi', 'kidnapped', 'one', 'year', 'ago', 'today'] 0\n",
            "['texas', 'execute', 'man', 'murdering', 'boy', 'drinking', 'blood'] 0\n",
            "['art', 'expert', 'confirm', 'guggenheim', 'museum', 'forgery'] 1\n",
            "['pregnant', 'woman', 'glow', 'rage'] 1\n",
            "['dad', 'strap', 'baby', 'steering', 'wheel', 'spin', 'around'] 0\n",
            "['u', 'driving', 'le', 'still', 'building', 'highway'] 0\n",
            "['independent', 'baking', 'scene', 'apparently', 'worth', 'documentary'] 1\n",
            "['george', 'rr', 'martin', 'promise', 'fan', 'wind', 'winter', 'nearly', 'started'] 1\n",
            "['historical', 'archive', 'twenty', 'top', 'book', 'print', 'present'] 1\n",
            "['20', 'people', 'found', 'refuge', 'famous', 'paris', 'bookstore', 'attack'] 0\n",
            "['area', 'dad', 'didnt', 'shell', '100', 'aquarium', 'lecture', 'ecosystem'] 1\n",
            "['area', 'man', 'proud', 'blood', 'type'] 1\n",
            "['member', 'opening', 'band', 'walking', 'among', 'crowd', 'intermission', 'like', 'god', 'among', 'men'] 1\n",
            "['natural', 'light', 'important', 'local', 'man'] 1\n",
            "['pelosi', 'throw', 'cold', 'water', 'tax', 'extenders', 'bill', 'talk', 'run', 'wire'] 0\n",
            "['15', 'photo', 'hot', 'dude', 'supporting', 'bernie', 'sander', 'make', 'feelthebern'] 0\n",
            "['woman', 'pay', 'full', 'price', 'carpet', 'one', 'day', 'non', 'sale'] 1\n",
            "['talkative', 'motherfucker', 'extroverted', 'friend', 'got', 'train'] 1\n",
            "['area', 'man', 'somehow', 'endures', 'harrowing', 'entertainment', 'free', 'commute'] 1\n",
            "['riverboat', 'horseracing', 'fails', 'utterly'] 1\n",
            "['new', 'york', 'time', 'editorial', 'board', 'endorses', 'john', 'kasich', 'gop', 'nomination'] 0\n",
            "['learning', 'right', 'give', 'might'] 0\n",
            "['jk', 'rowling', 'tweet', 'hilarious', 'response', 'confusing', 'olympic', 'sport'] 0\n",
            "['3', 'thing', 'know', 'learning', 'disability'] 0\n",
            "['proof', 'shouldnt', 'blame', 'teacher', 'achievement', 'gap'] 0\n",
            "['man', 'excited', 'look', 'like', 'different', 'type', 'idiot', 'front', 'coworkers', 'bar'] 1\n",
            "['humane', 'society', 'urge', 'american', 'opt', 'shelter', 'turkey', 'thanksgiving'] 1\n"
          ]
        }
      ]
    }
  ]
}